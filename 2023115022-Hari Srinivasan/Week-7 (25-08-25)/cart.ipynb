{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f54e2c",
   "metadata": {},
   "source": [
    "# CART Decision Tree Classifier for Iris Dataset\n",
    "\n",
    "This notebook implements the CART (Classification and Regression Trees) algorithm from scratch to classify iris flowers.\n",
    "\n",
    "## Algorithm Overview:\n",
    "- Uses **Gini Impurity** to measure node purity and select the best splits\n",
    "- Handles both continuous and categorical features\n",
    "- Creates binary splits at each node\n",
    "- More flexible than ID3 as it can work with numerical data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ff675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98985121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the iris dataset\n",
    "def load_iris_data():\n",
    "    \"\"\"Load and return iris dataset as pandas DataFrame\"\"\"\n",
    "    iris = load_iris()\n",
    "    \n",
    "    # Create DataFrame with original continuous features\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['species'] = iris.target_names[iris.target]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "iris_df = load_iris_data()\n",
    "print(\"Iris Dataset Shape:\", iris_df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(iris_df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(iris_df.info())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(iris_df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART Decision Tree Implementation\n",
    "class CARTDecisionTree:\n",
    "    \"\"\"Simple CART Decision Tree implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=10, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree = None\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def gini_impurity(self, labels):\n",
    "        \"\"\"Calculate Gini impurity of a set of labels\"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Count occurrences of each label\n",
    "        counts = Counter(labels)\n",
    "        total = len(labels)\n",
    "        \n",
    "        # Calculate Gini impurity\n",
    "        gini = 1.0\n",
    "        for count in counts.values():\n",
    "            p = count / total\n",
    "            gini -= p ** 2\n",
    "        \n",
    "        return gini\n",
    "    \n",
    "    def find_best_split(self, data, target_col):\n",
    "        \"\"\"Find the best feature and threshold to split on\"\"\"\n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_left_data = None\n",
    "        best_right_data = None\n",
    "        \n",
    "        # Get feature columns (excluding target)\n",
    "        feature_cols = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Try each feature\n",
    "        for feature in feature_cols:\n",
    "            # Get unique values for this feature and sort them\n",
    "            unique_values = sorted(data[feature].unique())\n",
    "            \n",
    "            # Try each possible threshold (midpoint between consecutive unique values)\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                \n",
    "                # Split data based on threshold\n",
    "                left_data = data[data[feature] <= threshold]\n",
    "                right_data = data[data[feature] > threshold]\n",
    "                \n",
    "                # Skip if split doesn't meet minimum sample requirements\n",
    "                if len(left_data) < self.min_samples_leaf or len(right_data) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate weighted Gini impurity\n",
    "                total_samples = len(data)\n",
    "                left_weight = len(left_data) / total_samples\n",
    "                right_weight = len(right_data) / total_samples\n",
    "                \n",
    "                weighted_gini = (left_weight * self.gini_impurity(left_data[target_col]) + \n",
    "                               right_weight * self.gini_impurity(right_data[target_col]))\n",
    "                \n",
    "                # Update best split if this is better\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_left_data = left_data\n",
    "                    best_right_data = right_data\n",
    "        \n",
    "        return best_feature, best_threshold, best_left_data, best_right_data, best_gini\n",
    "    \n",
    "    def build_tree(self, data, target_col, depth=0):\n",
    "        \"\"\"Recursively build the decision tree\"\"\"\n",
    "        # Base cases\n",
    "        target_values = data[target_col].unique()\n",
    "        \n",
    "        # If all samples have same class, return leaf node\n",
    "        if len(target_values) == 1:\n",
    "            return target_values[0]\n",
    "        \n",
    "        # If max depth reached or not enough samples, return most common class\n",
    "        if (depth >= self.max_depth or \n",
    "            len(data) < self.min_samples_split or \n",
    "            len(data) < 2 * self.min_samples_leaf):\n",
    "            return data[target_col].mode()[0]\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold, left_data, right_data, best_gini = self.find_best_split(data, target_col)\n",
    "        \n",
    "        # If no good split found, return most common class\n",
    "        if best_feature is None or best_gini == float('inf'):\n",
    "            return data[target_col].mode()[0]\n",
    "        \n",
    "        # Create tree node\n",
    "        tree = {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': None,\n",
    "            'right': None,\n",
    "            'gini': best_gini,\n",
    "            'samples': len(data),\n",
    "            'class_distribution': dict(data[target_col].value_counts())\n",
    "        }\n",
    "        \n",
    "        # Recursively build left and right subtrees\n",
    "        tree['left'] = self.build_tree(left_data, target_col, depth + 1)\n",
    "        tree['right'] = self.build_tree(right_data, target_col, depth + 1)\n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree\"\"\"\n",
    "        # Combine features and target\n",
    "        data = X.copy()\n",
    "        data['target'] = y\n",
    "        \n",
    "        self.feature_names = list(X.columns)\n",
    "        self.tree = self.build_tree(data, 'target')\n",
    "        return self\n",
    "    \n",
    "    def predict_single(self, sample, tree=None):\n",
    "        \"\"\"Predict class for a single sample\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "        \n",
    "        # If tree is a leaf node (string), return the class\n",
    "        if isinstance(tree, str):\n",
    "            return tree\n",
    "        \n",
    "        # Navigate based on feature threshold\n",
    "        feature_value = sample[tree['feature']]\n",
    "        \n",
    "        if feature_value <= tree['threshold']:\n",
    "            return self.predict_single(sample, tree['left'])\n",
    "        else:\n",
    "            return self.predict_single(sample, tree['right'])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for multiple samples\"\"\"\n",
    "        predictions = []\n",
    "        for _, sample in X.iterrows():\n",
    "            pred = self.predict_single(sample)\n",
    "            predictions.append(pred)\n",
    "        return predictions\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\"\", side=\"\"):\n",
    "        \"\"\"Print the decision tree structure\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "        \n",
    "        if isinstance(tree, str):\n",
    "            print(f\"{indent}{side}-> Class: {tree}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"{indent}{side}Feature: {tree['feature']} <= {tree['threshold']:.3f}\")\n",
    "        print(f\"{indent}  Gini: {tree['gini']:.3f}, Samples: {tree['samples']}\")\n",
    "        print(f\"{indent}  Class distribution: {tree['class_distribution']}\")\n",
    "        \n",
    "        if tree['left'] is not None:\n",
    "            self.print_tree(tree['left'], indent + \"  \", \"Left \")\n",
    "        if tree['right'] is not None:\n",
    "            self.print_tree(tree['right'], indent + \"  \", \"Right \")\n",
    "\n",
    "print(\"CART Decision Tree class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23551f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = iris_df.drop('species', axis=1)\n",
    "y = iris_df['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c29833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CART Decision Tree\n",
    "print(\"Training CART Decision Tree...\")\n",
    "\n",
    "# Create and train the model\n",
    "cart_tree = CARTDecisionTree(max_depth=5, min_samples_split=2, min_samples_leaf=1)\n",
    "cart_tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CART DECISION TREE STRUCTURE:\")\n",
    "print(\"=\"*60)\n",
    "cart_tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b184e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = cart_tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE PREDICTIONS:\")\n",
    "print(\"=\"*50)\n",
    "for i in range(min(8, len(X_test))):\n",
    "    sample = X_test.iloc[i]\n",
    "    actual = y_test.iloc[i]\n",
    "    predicted = y_pred[i]\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Sepal Length: {sample['sepal length (cm)']:.2f} cm\")\n",
    "    print(f\"  Sepal Width:  {sample['sepal width (cm)']:.2f} cm\")\n",
    "    print(f\"  Petal Length: {sample['petal length (cm)']:.2f} cm\")\n",
    "    print(f\"  Petal Width:  {sample['petal width (cm)']:.2f} cm\")\n",
    "    print(f\"  Actual: {actual}, Predicted: {predicted}\")\n",
    "    print(f\"  {'✓ Correct' if actual == predicted else '✗ Wrong'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results and tree analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test),\n",
    "            ax=axes[0,0])\n",
    "axes[0,0].set_title('Confusion Matrix')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# 2. Feature importance (based on splits in tree)\n",
    "def calculate_feature_importance(tree):\n",
    "    \"\"\"Calculate feature importance based on Gini improvement\"\"\"\n",
    "    importance = {}\n",
    "    \n",
    "    def traverse_tree(node, samples_weight=1.0):\n",
    "        if isinstance(node, str):\n",
    "            return\n",
    "        \n",
    "        feature = node['feature']\n",
    "        if feature not in importance:\n",
    "            importance[feature] = 0\n",
    "        \n",
    "        # Add importance based on Gini improvement weighted by samples\n",
    "        gini_improvement = node['gini'] * samples_weight\n",
    "        importance[feature] += gini_improvement\n",
    "        \n",
    "        # Recursively traverse subtrees\n",
    "        if node['left'] is not None:\n",
    "            left_weight = 0.5 * samples_weight  # Simplified weighting\n",
    "            traverse_tree(node['left'], left_weight)\n",
    "        if node['right'] is not None:\n",
    "            right_weight = 0.5 * samples_weight  # Simplified weighting\n",
    "            traverse_tree(node['right'], right_weight)\n",
    "    \n",
    "    traverse_tree(tree)\n",
    "    \n",
    "    # Normalize importance values\n",
    "    if importance:\n",
    "        total = sum(importance.values())\n",
    "        for feature in importance:\n",
    "            importance[feature] /= total\n",
    "    \n",
    "    return importance\n",
    "\n",
    "feature_importance = calculate_feature_importance(cart_tree.tree)\n",
    "\n",
    "if feature_importance:\n",
    "    features = list(feature_importance.keys())\n",
    "    importance_values = list(feature_importance.values())\n",
    "    \n",
    "    axes[0,1].bar(features, importance_values, color='skyblue')\n",
    "    axes[0,1].set_title('Feature Importance (CART)')\n",
    "    axes[0,1].set_xlabel('Features')\n",
    "    axes[0,1].set_ylabel('Importance')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[0,1].text(0.5, 0.5, 'No feature importance\\ncalculated', \n",
    "                   ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "    axes[0,1].set_title('Feature Importance')\n",
    "\n",
    "# 3. Decision boundary visualization (for 2 most important features)\n",
    "if len(feature_importance) >= 2:\n",
    "    # Get two most important features\n",
    "    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    feature1, feature2 = sorted_features[0][0], sorted_features[1][0]\n",
    "    \n",
    "    # Create a subset with just these two features for visualization\n",
    "    X_subset = iris_df[[feature1, feature2]]\n",
    "    y_subset = iris_df['species']\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_subset.iloc[:, 0].min() - 1, X_subset.iloc[:, 0].max() + 1\n",
    "    y_min, y_max = X_subset.iloc[:, 1].min() - 1, X_subset.iloc[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Train a simple CART tree on just these two features\n",
    "    simple_cart = CARTDecisionTree(max_depth=3)\n",
    "    simple_cart.fit(X_subset, y_subset)\n",
    "    \n",
    "    # Make predictions on the mesh grid\n",
    "    mesh_points = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=[feature1, feature2])\n",
    "    Z = simple_cart.predict(mesh_points)\n",
    "    \n",
    "    # Convert string labels to numeric for plotting\n",
    "    label_to_num = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
    "    Z_numeric = [label_to_num[label] for label in Z]\n",
    "    Z_numeric = np.array(Z_numeric).reshape(xx.shape)\n",
    "    \n",
    "    axes[1,0].contourf(xx, yy, Z_numeric, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot the data points\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for i, species in enumerate(['setosa', 'versicolor', 'virginica']):\n",
    "        mask = y_subset == species\n",
    "        axes[1,0].scatter(X_subset[mask].iloc[:, 0], X_subset[mask].iloc[:, 1], \n",
    "                         c=colors[i], label=species, alpha=0.7)\n",
    "    \n",
    "    axes[1,0].set_xlabel(feature1)\n",
    "    axes[1,0].set_ylabel(feature2)\n",
    "    axes[1,0].set_title('Decision Boundary (Top 2 Features)')\n",
    "    axes[1,0].legend()\n",
    "else:\n",
    "    axes[1,0].text(0.5, 0.5, 'Not enough features\\nfor boundary plot', \n",
    "                   ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "\n",
    "# 4. Tree depth analysis\n",
    "def get_tree_depth(tree):\n",
    "    \"\"\"Calculate the depth of the tree\"\"\"\n",
    "    if isinstance(tree, str):\n",
    "        return 0\n",
    "    \n",
    "    left_depth = get_tree_depth(tree['left']) if tree['left'] else 0\n",
    "    right_depth = get_tree_depth(tree['right']) if tree['right'] else 0\n",
    "    \n",
    "    return 1 + max(left_depth, right_depth)\n",
    "\n",
    "def count_nodes(tree):\n",
    "    \"\"\"Count total nodes in the tree\"\"\"\n",
    "    if isinstance(tree, str):\n",
    "        return 1\n",
    "    \n",
    "    left_count = count_nodes(tree['left']) if tree['left'] else 0\n",
    "    right_count = count_nodes(tree['right']) if tree['right'] else 0\n",
    "    \n",
    "    return 1 + left_count + right_count\n",
    "\n",
    "tree_depth = get_tree_depth(cart_tree.tree)\n",
    "total_nodes = count_nodes(cart_tree.tree)\n",
    "\n",
    "# Tree statistics\n",
    "stats_text = f\"\"\"Tree Statistics:\n",
    "• Max Depth: {tree_depth}\n",
    "• Total Nodes: {total_nodes}\n",
    "• Leaf Nodes: {total_nodes - (total_nodes // 2)}\n",
    "• Test Accuracy: {accuracy:.4f}\n",
    "\n",
    "CART vs ID3:\n",
    "• Uses Gini Impurity\n",
    "• Handles continuous features\n",
    "• Creates binary splits\n",
    "• More flexible than ID3\"\"\"\n",
    "\n",
    "axes[1,1].text(0.1, 0.9, stats_text, transform=axes[1,1].transAxes, \n",
    "               verticalalignment='top', fontsize=11, fontfamily='monospace',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "axes[1,1].set_title('Tree Analysis')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results Summary:\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Algorithm: CART Decision Tree\")\n",
    "print(f\"Dataset: Iris (continuous features)\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Tree Depth: {tree_depth}\")\n",
    "print(f\"Total Nodes: {total_nodes}\")\n",
    "print(f\"Total samples: {len(iris_df)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features used: {len(X.columns)}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")\n",
    "if feature_importance:\n",
    "    print(f\"Most important feature: {max(feature_importance, key=feature_importance.get)}\")\n",
    "print(\"✓ CART implementation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
