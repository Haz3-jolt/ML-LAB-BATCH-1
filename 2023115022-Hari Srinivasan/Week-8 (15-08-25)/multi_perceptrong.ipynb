{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4327f8cd",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron ANN for Iris Dataset\n",
    "\n",
    "This notebook implements a simple multi-layer perceptron (MLP) neural network to classify all 3 iris species.\n",
    "\n",
    "## MLP Overview:\n",
    "- **Multi-layer neural network** with hidden layers\n",
    "- Uses **sigmoid activation** for smooth gradients\n",
    "- Can solve **non-linearly separable** problems\n",
    "- **Backpropagation** algorithm for training\n",
    "- Handles **multi-class classification** (3 iris species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8eb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the iris dataset\n",
    "def load_iris_data():\n",
    "    \"\"\"Load iris dataset for multi-class classification\"\"\"\n",
    "    iris = load_iris()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['species'] = iris.target_names[iris.target]\n",
    "    df['target'] = iris.target  # Numeric target (0, 1, 2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "iris_df = load_iris_data()\n",
    "print(\"Iris Dataset Shape:\", iris_df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(iris_df.head())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(iris_df['species'].value_counts())\n",
    "print(\"\\nNumeric targets:\")\n",
    "print(\"Setosa (0):\", sum(iris_df['target'] == 0))\n",
    "print(\"Versicolor (1):\", sum(iris_df['target'] == 1))\n",
    "print(\"Virginica (2):\", sum(iris_df['target'] == 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Multi-Layer Perceptron Implementation\n",
    "class MultiLayerPerceptron:\n",
    "    \"\"\"Simple MLP for multi-class classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights randomly (small values)\n",
    "        self.weights_input_hidden = np.random.uniform(-0.5, 0.5, (input_size, hidden_size))\n",
    "        self.weights_hidden_output = np.random.uniform(-0.5, 0.5, (hidden_size, output_size))\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "        \n",
    "        # Track training history\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def one_hot_encode(self, y, num_classes):\n",
    "        \"\"\"Convert labels to one-hot encoding\"\"\"\n",
    "        encoded = np.zeros((len(y), num_classes))\n",
    "        for i, label in enumerate(y):\n",
    "            encoded[i, label] = 1\n",
    "        return encoded\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # Input to hidden layer\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Hidden to output layer\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = self.sigmoid(self.output_input)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward_pass(self, X, y, output):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Calculate output layer error\n",
    "        output_error = y - output\n",
    "        output_delta = output_error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        # Calculate hidden layer error\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * self.learning_rate / m\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate / m\n",
    "        \n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta) * self.learning_rate / m\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate / m\n",
    "    \n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate mean squared error loss\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"Train the MLP\"\"\"\n",
    "        # Convert labels to one-hot encoding\n",
    "        y_one_hot = self.one_hot_encode(y, self.output_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward_pass(X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.calculate_loss(y_one_hot, output)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward_pass(X, y_one_hot, output)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        output = self.forward_pass(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        return self.forward_pass(X)\n",
    "\n",
    "print(\"Multi-Layer Perceptron class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X = iris_df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "y = iris_df['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "for i, species in enumerate(['Setosa', 'Versicolor', 'Virginica']):\n",
    "    count = sum(y_train == i)\n",
    "    print(f\"{species}: {count}\")\n",
    "    \n",
    "print(f\"\\nTest set target distribution:\")\n",
    "for i, species in enumerate(['Setosa', 'Versicolor', 'Virginica']):\n",
    "    count = sum(y_test == i)\n",
    "    print(f\"{species}: {count}\")\n",
    "\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "print(f\"Network architecture will be: 4 -> ? -> 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Multi-Layer Perceptron\n",
    "print(\"Training Multi-Layer Perceptron...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create MLP with simple architecture: 4 inputs -> 6 hidden -> 3 outputs\n",
    "mlp = MultiLayerPerceptron(\n",
    "    input_size=4,      # 4 iris features\n",
    "    hidden_size=6,     # 6 hidden neurons (simple)\n",
    "    output_size=3,     # 3 iris species\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "print(f\"Network Architecture: 4 -> 6 -> 3\")\n",
    "print(f\"Total parameters: {4*6 + 6 + 6*3 + 3} (weights + biases)\")\n",
    "print(\"Starting training...\")\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train.values, epochs=500, verbose=True)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = mlp.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_test_proba = mlp.predict_proba(X_test_scaled)\n",
    "\n",
    "print(f\"\\nPerformance Results:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Training Loss: {mlp.loss_history[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*50)\n",
    "target_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
    "\n",
    "# Show some example predictions with probabilities\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE PREDICTIONS WITH PROBABILITIES:\")\n",
    "print(\"=\"*50)\n",
    "for i in range(min(8, len(X_test))):\n",
    "    actual = y_test.iloc[i]\n",
    "    predicted = y_test_pred[i]\n",
    "    proba = y_test_proba[i]\n",
    "    features = X_test.iloc[i]\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Features: SL={features[0]:.2f}, SW={features[1]:.2f}, PL={features[2]:.2f}, PW={features[3]:.2f}\")\n",
    "    print(f\"  Actual: {target_names[actual]}\")\n",
    "    print(f\"  Predicted: {target_names[predicted]}\")\n",
    "    print(f\"  Probabilities: Setosa={proba[0]:.3f}, Versicolor={proba[1]:.3f}, Virginica={proba[2]:.3f}\")\n",
    "    print(f\"  {'✓ Correct' if actual == predicted else '✗ Wrong'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results and training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Training History (Loss and Accuracy)\n",
    "epochs = range(1, len(mlp.loss_history) + 1)\n",
    "\n",
    "ax1 = axes[0,0]\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(epochs, mlp.loss_history, color=color, linewidth=2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Accuracy', color=color)\n",
    "ax2.plot(epochs, mlp.accuracy_history, color=color, linewidth=2)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "axes[0,0].set_title('Training Progress: Loss and Accuracy')\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names,\n",
    "            ax=axes[0,1])\n",
    "axes[0,1].set_title('Confusion Matrix')\n",
    "axes[0,1].set_xlabel('Predicted')\n",
    "axes[0,1].set_ylabel('Actual')\n",
    "\n",
    "# 3. Prediction Confidence Distribution\n",
    "confidence_scores = np.max(y_test_proba, axis=1)  # Max probability for each prediction\n",
    "correct_predictions = (y_test.values == y_test_pred)\n",
    "\n",
    "axes[1,0].hist(confidence_scores[correct_predictions], bins=15, alpha=0.7, \n",
    "               label='Correct', color='green', density=True)\n",
    "axes[1,0].hist(confidence_scores[~correct_predictions], bins=15, alpha=0.7, \n",
    "               label='Incorrect', color='red', density=True)\n",
    "axes[1,0].set_xlabel('Prediction Confidence (Max Probability)')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].set_title('Prediction Confidence Distribution')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Network Architecture and Summary\n",
    "network_info = f\"\"\"Multi-Layer Perceptron Summary:\n",
    "\n",
    "Architecture:\n",
    "• Input Layer:    4 neurons (iris features)\n",
    "• Hidden Layer:   6 neurons (sigmoid)\n",
    "• Output Layer:   3 neurons (sigmoid)\n",
    "• Total Parameters: {4*6 + 6 + 6*3 + 3}\n",
    "\n",
    "Training Configuration:\n",
    "• Learning Rate:  {mlp.learning_rate}\n",
    "• Epochs:         {len(mlp.loss_history)}\n",
    "• Activation:     Sigmoid\n",
    "• Loss Function:  Mean Squared Error\n",
    "\n",
    "Performance:\n",
    "• Training Accuracy: {train_accuracy:.4f}\n",
    "• Test Accuracy:     {test_accuracy:.4f}\n",
    "• Final Loss:        {mlp.loss_history[-1]:.4f}\n",
    "\n",
    "Key Features:\n",
    "• Handles multi-class classification\n",
    "• Backpropagation learning\n",
    "• Non-linear decision boundaries\n",
    "• Probability outputs\"\"\"\n",
    "\n",
    "axes[1,1].text(0.05, 0.95, network_info, transform=axes[1,1].transAxes, \n",
    "               verticalalignment='top', fontsize=10, fontfamily='monospace',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "axes[1,1].set_title('Network Summary')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display weight matrices (simplified view)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NETWORK WEIGHTS (SIMPLIFIED VIEW)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Input to Hidden Layer weights shape:\", mlp.weights_input_hidden.shape)\n",
    "print(\"Hidden to Output Layer weights shape:\", mlp.weights_hidden_output.shape)\n",
    "print(\"\\nHidden Layer Biases:\", mlp.bias_hidden.round(3))\n",
    "print(\"Output Layer Biases:\", mlp.bias_output.round(3))\n",
    "\n",
    "print(f\"\\nFinal Results Summary:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: Multi-Layer Perceptron ANN\")\n",
    "print(f\"Problem: Multi-class Classification (3 iris species)\")\n",
    "print(f\"Architecture: 4 -> 6 -> 3\")\n",
    "print(f\"Features: 4 iris measurements (standardized)\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Training Epochs: {len(mlp.loss_history)}\")\n",
    "print(f\"Final Loss: {mlp.loss_history[-1]:.4f}\")\n",
    "print(\"✓ Multi-Layer Perceptron implementation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
