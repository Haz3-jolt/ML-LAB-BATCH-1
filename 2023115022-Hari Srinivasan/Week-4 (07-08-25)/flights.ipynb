{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIOUBvC9GX2Y"
   },
   "source": [
    "Preprocessing Steps -Binning Algorithms,.min-max..Normalization Techniques, Hypothesis Testing, ChiSquare Test, Confusion Matrix,\n",
    "Implement Dimensionality reduction using Principle component Analysis method on a dataset iris\n",
    "Implement and demonstrate the FIND-S algorithm for finding the most specific hypothesis based on a given set of training data samples. Read the training data from a .CSV file.\n",
    "For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1754297955198,
     "user": {
      "displayName": "haze eshwald",
      "userId": "01750877828677052398"
     },
     "user_tz": -330
    },
    "id": "D4Nxiz-qF03J",
    "outputId": "142b1f6b-7fca-4896-b9e2-4f6fffba2ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Origin', 'Destination', 'Company', 'Departure Time', 'Arrival Time', 'Duration Time', 'Flight Price', 'Date', 'Cabin Class']\n",
      "First few rows:\n",
      "  Origin Destination   Company Departure Time Arrival Time Duration Time  \\\n",
      "0    BOM         DEL   IndiGo           08:30        10:25        1h 55m   \n",
      "1    BOM         DEL   IndiGo           07:45        09:50        2h 05m   \n",
      "2    BOM         DEL  Vistara           12:25        14:30        2h 05m   \n",
      "3    BOM         DEL   IndiGo           10:05        12:15        2h 10m   \n",
      "4    BOM         DEL   IndiGo           13:40        15:50        2h 10m   \n",
      "\n",
      "  Flight Price        Date Cabin Class  \n",
      "0        6,153  14-02-2022     Economy  \n",
      "1        5,943  14-02-2022     Economy  \n",
      "2        6,249  14-02-2022     Economy  \n",
      "3        5,943  14-02-2022     Economy  \n",
      "4        5,943  14-02-2022     Economy  \n",
      "\n",
      "--- Hypothesis Testing (Simple Mean Comparison) ---\n",
      "Mean duration for SpiceJet: nan\n",
      "Mean duration for IndiGo: nan\n",
      "Note: This is a basic comparison, not a formal statistical hypothesis test.\n",
      "\n",
      "--- Chi-Square Test (Manual Implementation Example) ---\n",
      "Contingency Table:\n",
      "Cabin Class     Economy\n",
      "Company                \n",
      "Air India           343\n",
      "AirAsia India       104\n",
      "Go First            535\n",
      "IndiGo              976\n",
      "SpiceJet            150\n",
      "Vistara             799\n",
      "\n",
      "Calculated Chi-Square Statistic: 0.00\n",
      "\n",
      "--- Confusion Matrix (Manual Implementation Example) ---\n",
      "Confusion Matrix:\n",
      "          Business  Economy\n",
      "Business        18       23\n",
      "Economy         35       24\n",
      "\n",
      "Accuracy: 0.42\n",
      "Precision: 0.51\n",
      "Recall: 0.41\n",
      "F1-Score: 0.45\n",
      "\n",
      "--- Principal Component Analysis (PCA) on Iris Dataset ---\n",
      "Original data shape: (150, 4)\n",
      "Transformed data shape: (150, 2)\n",
      "First 5 rows of transformed data:\n",
      "[[-2.26470281 -0.4800266 ]\n",
      " [-2.08096115  0.67413356]\n",
      " [-2.36422905  0.34190802]\n",
      " [-2.29938422  0.59739451]\n",
      " [-2.38984217 -0.64683538]]\n",
      "\n",
      "--- FIND-S Algorithm ---\n",
      "\n",
      "'finds_training_data.csv' not found. Skipping FIND-S algorithm.\n",
      "\n",
      "--- Candidate-Elimination Algorithm ---\n",
      "\n",
      "'candidate_elimination_training_data.csv' not found. Skipping Candidate-Elimination algorithm.\n",
      "\n",
      "Skipping data processing and algorithm implementations due to dataset not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haz3/codel/college/IT23504-Machine-Learning/.venv/lib/python3.13/site-packages/numpy/lib/_nanfunctions_impl.py:1214: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/tmp/ipykernel_35492/2211511155.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Flight Price'].fillna(10000, inplace=True)\n",
      "/home/haz3/codel/college/IT23504-Machine-Learning/.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:686: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/haz3/codel/college/IT23504-Machine-Learning/.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:706: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "/tmp/ipykernel_35492/2211511155.py:83: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  expected_frequency = (row_sums[i] * col_sums[j]) / total_sum\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('dataset/airlines.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please make sure 'airlines_flights_data.csv' is in the correct directory.\")\n",
    "    df = None # Set df to None if file not found\n",
    "\n",
    "# Check if the dataset was loaded successfully\n",
    "if df is not None:\n",
    "    # --- Preprocessing Steps ---\n",
    "    \n",
    "    # Display available columns\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    print(\"First few rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 1. Binning Algorithms (Example: Equal-width binning for 'Flight Price')\n",
    "    # Convert 'Flight Price' to numeric, forcing errors to NaN\n",
    "    df['Flight Price'] = pd.to_numeric(df['Flight Price'], errors='coerce')\n",
    "    \n",
    "    # Fill NaN values with median price to ensure binning works\n",
    "    median_price = df['Flight Price'].median()\n",
    "    if pd.isna(median_price):\n",
    "        # If all prices are NaN, use a default value\n",
    "        df['Flight Price'].fillna(10000, inplace=True)\n",
    "    else:\n",
    "        df['Flight Price'].fillna(median_price, inplace=True)\n",
    "    \n",
    "    # Now perform binning\n",
    "    df['price_binned'] = pd.cut(df['Flight Price'], bins=5, labels=False)\n",
    "\n",
    "    # 2. Min-Max Normalization Techniques (Example: 'Duration Time')\n",
    "    scaler = MinMaxScaler()\n",
    "    # Convert Duration Time to numeric first\n",
    "    df['Duration Time'] = pd.to_numeric(df['Duration Time'], errors='coerce')\n",
    "    df['duration_normalized'] = scaler.fit_transform(df[['Duration Time']])\n",
    "\n",
    "    # --- Hypothesis Testing (Example: Comparing mean duration for two airlines) ---\n",
    "    # This is a basic example, a proper hypothesis test requires more rigorous statistical methods\n",
    "    # and checking of assumptions.\n",
    "    airline1 = 'SpiceJet'\n",
    "    airline2 = 'IndiGo'\n",
    "\n",
    "    duration_airline1 = df[df['Company'] == airline1]['Duration Time']\n",
    "    duration_airline2 = df[df['Company'] == airline2]['Duration Time']\n",
    "\n",
    "    # Simple comparison of means\n",
    "    mean_duration_airline1 = duration_airline1.mean()\n",
    "    mean_duration_airline2 = duration_airline2.mean()\n",
    "\n",
    "    print(f\"\\n--- Hypothesis Testing (Simple Mean Comparison) ---\")\n",
    "    print(f\"Mean duration for {airline1}: {mean_duration_airline1:.2f}\")\n",
    "    print(f\"Mean duration for {airline2}: {mean_duration_airline2:.2f}\")\n",
    "    print(\"Note: This is a basic comparison, not a formal statistical hypothesis test.\")\n",
    "\n",
    "\n",
    "    # --- Chi-Square Test (Manual Implementation Example) ---\n",
    "    # This example performs a chi-square test of independence between 'Company' and 'Cabin Class'.\n",
    "    # A more complete implementation would involve calculating expected frequencies and the chi-square statistic.\n",
    "    print(f\"\\n--- Chi-Square Test (Manual Implementation Example) ---\")\n",
    "    contingency_table = pd.crosstab(df['Company'], df['Cabin Class'])\n",
    "    print(\"Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "\n",
    "    # Manual Chi-Square calculation\n",
    "    chi2_statistic = 0\n",
    "    rows = contingency_table.shape[0]\n",
    "    cols = contingency_table.shape[1]\n",
    "    row_sums = contingency_table.sum(axis=1)\n",
    "    col_sums = contingency_table.sum(axis=0)\n",
    "    total_sum = contingency_table.sum().sum()\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            observed_frequency = contingency_table.iloc[i, j]\n",
    "            expected_frequency = (row_sums[i] * col_sums[j]) / total_sum\n",
    "            if expected_frequency != 0:\n",
    "                chi2_statistic += ((observed_frequency - expected_frequency) ** 2) / expected_frequency\n",
    "\n",
    "    print(f\"\\nCalculated Chi-Square Statistic: {chi2_statistic:.2f}\")\n",
    "\n",
    "\n",
    "    # --- Confusion Matrix (Manual Implementation Example) ---\n",
    "    # This requires a classification task and predicted vs actual values.\n",
    "    # As we don't have a classification model trained yet, we will create a dummy example\n",
    "    print(f\"\\n--- Confusion Matrix (Manual Implementation Example) ---\")\n",
    "\n",
    "    # Create dummy true and predicted labels for demonstration\n",
    "    # In a real scenario, these would come from a trained classification model\n",
    "    true_labels = np.random.choice(['Economy', 'Business'], size=100)\n",
    "    predicted_labels = np.random.choice(['Economy', 'Business'], size=100)\n",
    "\n",
    "    # Define unique classes\n",
    "    classes = sorted(list(set(true_labels) | set(predicted_labels)))\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Initialize confusion matrix\n",
    "    conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    # Populate confusion matrix\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        true_idx = classes.index(true)\n",
    "        pred_idx = classes.index(pred)\n",
    "        conf_matrix[true_idx, pred_idx] += 1\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(pd.DataFrame(conf_matrix, index=classes, columns=classes))\n",
    "\n",
    "    # Calculate metrics from confusion matrix (example for binary classification)\n",
    "    if n_classes == 2:\n",
    "        tn = conf_matrix[0, 0]\n",
    "        fp = conf_matrix[0, 1]\n",
    "        fn = conf_matrix[1, 0]\n",
    "        tp = conf_matrix[1, 1]\n",
    "\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "        print(f\"Precision: {precision:.2f}\")\n",
    "        print(f\"Recall: {recall:.2f}\")\n",
    "        print(f\"F1-Score: {f1_score:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nMetrics like Accuracy, Precision, Recall, F1-Score are more commonly calculated for binary classification.\")\n",
    "\n",
    "# --- Implement Dimensionality reduction using Principle component Analysis method on a dataset iris ---\n",
    "\n",
    "print(f\"\\n--- Principal Component Analysis (PCA) on Iris Dataset ---\")\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "mean = np.mean(X_iris, axis=0)\n",
    "std = np.std(X_iris, axis=0)\n",
    "X_scaled = (X_iris - mean) / std\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort eigenvectors by decreasing eigenvalues\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose the number of components (e.e., 2 for visualization)\n",
    "n_components = 2\n",
    "principal_components = sorted_eigenvectors[:, :n_components]\n",
    "\n",
    "# Project the data onto the principal components\n",
    "X_pca = np.dot(X_scaled, principal_components)\n",
    "\n",
    "# Display the first few rows of the transformed data\n",
    "print(\"Original data shape:\", X_iris.shape)\n",
    "print(\"Transformed data shape:\", X_pca.shape)\n",
    "print(\"First 5 rows of transformed data:\")\n",
    "print(X_pca[:5])\n",
    "\n",
    "\n",
    "# --- Implement and demonstrate the FIND-S algorithm ---\n",
    "\n",
    "print(f\"\\n--- FIND-S Algorithm ---\")\n",
    "# Load training data from a CSV file (assuming 'finds_training_data.csv' exists)\n",
    "# Example structure of 'finds_training_data.csv': attribute1,attribute2,...,target_concept\n",
    "try:\n",
    "    finds_df = pd.read_csv('/datasets/finds_training_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n'finds_training_data.csv' not found. Skipping FIND-S algorithm.\")\n",
    "    finds_df = None # Set to None if file not found\n",
    "\n",
    "if finds_df is not None:\n",
    "    # Assuming the last column is the target concept (e.e., 'EnjoySport')\n",
    "    concepts = finds_df.iloc[:, :-1].values\n",
    "    targets = finds_df.iloc[:, -1].values\n",
    "\n",
    "    # Initialize the most specific hypothesis\n",
    "    # Use the first positive example if available, otherwise initialize with the most specific hypothesis\n",
    "    initial_hypothesis = None\n",
    "    for i in range(len(targets)):\n",
    "        if targets[i] == 'Yes': # Assuming 'Yes' is the positive class\n",
    "            initial_hypothesis = concepts[i].copy()\n",
    "            break\n",
    "\n",
    "    if initial_hypothesis is None:\n",
    "        print(\"No positive examples found in the training data for FIND-S.\")\n",
    "    else:\n",
    "        hypothesis = initial_hypothesis\n",
    "\n",
    "        # Iterate through the training examples\n",
    "        for i in range(len(concepts)):\n",
    "            if targets[i] == 'Yes':\n",
    "                for j in range(len(hypothesis)):\n",
    "                    # If the hypothesis attribute is not specific enough, generalize it\n",
    "                    if hypothesis[j] != concepts[i][j]:\n",
    "                        hypothesis[j] = '?'\n",
    "\n",
    "        print(\"Most specific hypothesis:\", hypothesis)\n",
    "\n",
    "\n",
    "# --- Implement and demonstrate the Candidate-Elimination algorithm ---\n",
    "\n",
    "print(f\"\\n--- Candidate-Elimination Algorithm ---\")\n",
    "# Load training data from a CSV file (assuming 'candidate_elimination_training_data.csv' exists)\n",
    "# Example structure of 'candidate_elimination_training_data.csv': attribute1,attribute2,...,target_concept\n",
    "try:\n",
    "    ce_df = pd.read_csv('/datasets/candidate_elimination_training_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n'candidate_elimination_training_data.csv' not found. Skipping Candidate-Elimination algorithm.\")\n",
    "    ce_df = None # Set to None if file not found\n",
    "\n",
    "if ce_df is not None:\n",
    "    # Assuming the last column is the target concept\n",
    "    ce_concepts = ce_df.iloc[:, :-1].values\n",
    "    ce_targets = ce_df.iloc[:, -1].values\n",
    "\n",
    "    # Initialize the general and specific boundary sets\n",
    "    # Assuming attributes can take any value from the training data or '?'\n",
    "    attribute_values = defaultdict(set)\n",
    "    for concept in ce_concepts:\n",
    "        for i, attr in enumerate(concept):\n",
    "            attribute_values[i].add(attr)\n",
    "\n",
    "    # Initialize G to the most general hypothesis (all '?')\n",
    "    G = {tuple(['?' for _ in range(ce_concepts.shape[1])])}\n",
    "\n",
    "    # Initialize S to the most specific hypothesis (empty set or the first positive example)\n",
    "    S = set()\n",
    "    for i in range(len(ce_targets)):\n",
    "        if ce_targets[i] == 'Yes': # Assuming 'Yes' is the positive class\n",
    "            S.add(tuple(ce_concepts[i]))\n",
    "            break\n",
    "    if not S:\n",
    "         # If no positive examples, S can remain empty or be initialized differently depending on the problem.\n",
    "         # For simplicity here, we'll assume there's at least one positive example in a valid dataset.\n",
    "         # A more robust implementation would handle this case.\n",
    "         print(\"Warning: No positive examples found for Candidate-Elimination. S remains empty.\")\n",
    "\n",
    "\n",
    "    # Helper function to check if hypothesis h is more general than hypothesis h1\n",
    "    def is_more_general(h, h1):\n",
    "        for i in range(len(h)):\n",
    "            if h[i] != '?' and h[i] != h1[i]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Helper function to check if hypothesis h is consistent with example (x, target)\n",
    "    def is_consistent(h, x, target):\n",
    "        match = True\n",
    "        for i in range(len(h)):\n",
    "            if h[i] != '?' and h[i] != x[i]:\n",
    "                match = False\n",
    "                break\n",
    "        if match and target == 'Yes':\n",
    "            return True\n",
    "        elif not match and target == 'No':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    # Iterate through the training examples\n",
    "    for i in range(len(ce_concepts)):\n",
    "        x = ce_concepts[i]\n",
    "        target = ce_targets[i]\n",
    "\n",
    "        if target == 'Yes':\n",
    "            # Remove from G any hypothesis inconsistent with the positive example\n",
    "            G = {g for g in G if is_consistent(g, x, target)}\n",
    "\n",
    "            # For each hypothesis s in S inconsistent with the positive example\n",
    "            S_new = set()\n",
    "            for s in S:\n",
    "                if not is_consistent(s, x, target):\n",
    "                    # Remove s from S and add minimal generalizations of s that are consistent\n",
    "                    # and more specific than some hypothesis in G\n",
    "                    for j in range(len(s)):\n",
    "                        if s[j] == '?':\n",
    "                            continue # Cannot specialize '?'\n",
    "                        if s[j] != x[j]:\n",
    "                            # Create a generalization by replacing s[j] with '?'\n",
    "                            s_generalized = list(s)\n",
    "                            s_generalized[j] = '?'\n",
    "                            s_generalized = tuple(s_generalized)\n",
    "\n",
    "                            # Add the generalized hypothesis if it's more specific than some g in G\n",
    "                            # and not more general than any other hypothesis in S\n",
    "                            is_minimal_generalization = True\n",
    "                            for g in G:\n",
    "                                if is_more_general(s_generalized, g):\n",
    "                                     # Check if it's not more general than any other hypothesis in S\n",
    "                                    for other_s in S_new.union(S - {s}): # Check against new and existing S hypotheses (excluding current s)\n",
    "                                        if is_more_general(s_generalized, other_s):\n",
    "                                            is_minimal_generalization = False\n",
    "                                            break\n",
    "                                    if is_minimal_generalization:\n",
    "                                         S_new.add(s_generalized)\n",
    "                                    break # Found a g that is more general, no need to check others\n",
    "                            # If no g is more general, this generalization is not valid in this context\n",
    "                            # based on the standard CE algorithm update for positive examples.\n",
    "                            # The standard update adds minimal generalizations that are consistent\n",
    "                            # and more specific than *some* hypothesis in G.\n",
    "                else:\n",
    "                    S_new.add(s) # If consistent, keep s\n",
    "\n",
    "            S = S_new\n",
    "            # Remove from S any hypothesis that is more general than another hypothesis in S\n",
    "            S = {s for s in S if not any(is_more_general(s, s1) for s1 in S if s != s1)}\n",
    "\n",
    "\n",
    "        elif target == 'No':\n",
    "            # Remove from S any hypothesis inconsistent with the negative example\n",
    "            S = {s for s in S if is_consistent(s, x, target)}\n",
    "\n",
    "            # For each hypothesis g in G inconsistent with the negative example\n",
    "            G_new = set()\n",
    "            for g in G:\n",
    "                if not is_consistent(g, x, target):\n",
    "                    # Remove g from G and add minimal specializations of g that are consistent\n",
    "                    # and more general than some hypothesis in S\n",
    "                    for j in range(len(g)):\n",
    "                        if g[j] == '?':\n",
    "                            # Create a specialization by replacing '?' with each possible attribute value\n",
    "                            for value in attribute_values[j]:\n",
    "                                g_specialized = list(g)\n",
    "                                g_specialized[j] = value\n",
    "                                g_specialized = tuple(g_specialized)\n",
    "\n",
    "                                # Add the specialized hypothesis if it's consistent and more general than some s in S\n",
    "                                is_minimal_specialization = True\n",
    "                                for s in S:\n",
    "                                    if is_more_general(g_specialized, s):\n",
    "                                        # Check if it's not more specific than any other hypothesis in G\n",
    "                                        for other_g in G_new.union(G - {g}): # Check against new and existing G hypotheses (excluding current g)\n",
    "                                            if is_more_general(other_g, g_specialized): # Note the order for specialization check\n",
    "                                                is_minimal_specialization = False\n",
    "                                                break\n",
    "                                        if is_minimal_specialization:\n",
    "                                             G_new.add(g_specialized)\n",
    "                                        break # Found an s that is more specific, no need to check others\n",
    "                else:\n",
    "                    G_new.add(g) # If consistent, keep g\n",
    "\n",
    "            G = G_new\n",
    "            # Remove from G any hypothesis that is more specific than another hypothesis in G\n",
    "            G = {g for g in G if not any(is_more_general(g1, g) for g1 in G if g != g1)}\n",
    "\n",
    "\n",
    "    print(\"Final Specific Boundary (S):\", S)\n",
    "    print(\"Final General Boundary (G):\", G)\n",
    "else:\n",
    "    print(\"\\nSkipping data processing and algorithm implementations due to dataset not found.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKaZZZvPNlLECSpfQqrunk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "it23504-machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
